{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started\n"
     ]
    }
   ],
   "source": [
    "print(\"Started\")\n",
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# # WWTP Prediction\n",
    "# ###### Made by Nitzan Farhi\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from keras.optimizers import SGD\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tqdm import tqdm\n",
    "import metrics\n",
    "import preprocess\n",
    "from metrics import *\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import lstme\n",
    "import importlib\n",
    "#import sherpa\n",
    "import csv\n",
    "importlib.reload(lstme)\n",
    "import math\n",
    "import models\n",
    "importlib.reload(lstme)\n",
    "importlib.reload(models)\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = preprocess.preprocess_df(clabel.AMMONIA, graphs=True)\n",
    "#df.to_pickle(\"temp_dataframe.pkl\") \n",
    "\n",
    "#import pandas as pd\n",
    "#pd.set_option('display.max_rows', 500)\n",
    "#pd.set_option('display.max_columns', 500)\n",
    "#pd.set_option('display.width', 1000)\n",
    "#df.describe()\n",
    "\n",
    "sns.set(style='white', context='notebook', palette='deep')\n",
    "# Seed value\n",
    "seed_value = 0\n",
    "os.environ['PYTHONHASHSEED'] = str(seed_value)\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "\n",
    "Ytest, Yhat = [], []\n",
    "\n",
    "# In[439]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def normalize(X, Y,binary=False):\n",
    "    scalers = list()\n",
    "    for i in tqdm(range(X.shape[1])):\n",
    "        scalers.append(MinMaxScaler())\n",
    "        X[:, i, :] = scalers[i].fit_transform(X[:, i, :])\n",
    "\n",
    "    scalers.append(MinMaxScaler())\n",
    "    if not binary:\n",
    "        Y = scalers[i + 1].fit_transform(Y)\n",
    "    return X, Y, scalers\n",
    "\n",
    "\n",
    "def normalize_2(x,y, binary=False):\n",
    "    x_min = x.min(axis=(0, 1), keepdims=True)\n",
    "    x_max = x.max(axis=(0, 1), keepdims=True)\n",
    "    x = (x - x_min)/(x_max - x_min)\n",
    "    y_min = y.min(axis=(0, 1), keepdims=True)\n",
    "    y_max = y.max(axis=(0, 1), keepdims=True)\n",
    "    y = (y - y_min)/(y_max -y_min)\n",
    "    \n",
    "    return np.nan_to_num(x),np.nan_to_num(y),(x_min,x_max,y_min,y_max)\n",
    "\n",
    "# Assert values are fine\n",
    "def check_boundaries(vals, min_val, max_val):\n",
    "    # print(np.amin(vals),np.amax(vals))\n",
    "    if np.amin(vals) >= min_val and np.amax(vals) <= max_val:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def oversampler(conX,conY):\n",
    "    shape2 = conX.shape[2]\n",
    "    conX = conX.reshape(conX.shape[0],-1)  # CNN_LSTM\n",
    "    oversample = RandomOverSampler(sampling_strategy='minority')\n",
    "    # fit and apply the transform\n",
    "    conX, conY = oversample.fit_resample(conX, conY)\n",
    "    conX = conX.reshape(conX.shape[0],-1,shape2)\n",
    "    return conX,conY\n",
    "\n",
    "def series_to_supervised(sequences, n_steps_in=1, n_steps_out=1, steps = 1, binary=False):\n",
    "    X, Y = list(), list()\n",
    "    print(\"LIMIT IS: \",ammonia_limit,\"Steps are: \",steps,\" Size is \",len(sequences))\n",
    "    \n",
    "    for i in range(0, len(sequences),steps):\n",
    "        # find the end of this pattern\n",
    "        end_ix = i + n_steps_in\n",
    "        out_end_ix = end_ix + n_steps_out\n",
    "        # check if we are beyond the dataset\n",
    "        if out_end_ix > len(sequences):\n",
    "            break\n",
    "        # gather input and output parts of the pattern\n",
    "        seq_x, seq_y = sequences[i:end_ix, :], sequences[out_end_ix - 1:out_end_ix][:, 0]\n",
    "        \n",
    "        if binary:\n",
    "            seq_y = [float(seq_y > ammonia_limit)]\n",
    "        if True or sufficient_feed_flow(seq_x):\n",
    "            X.append(seq_x)\n",
    "            Y.append(seq_y)\n",
    "    return np.array(X), np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_one_hot(ret_df):    \n",
    "    day_names = pd.get_dummies(ret_df.index.weekday,prefix='weekday')\n",
    "    hours = pd.get_dummies(ret_df.index.hour, prefix='hour')\n",
    "    ret_df = ret_df.reset_index()\n",
    "    tmp = pd.concat((ret_df, hours, day_names), axis=1)\n",
    "    \n",
    "    ret_df = ret_df.reset_index()\n",
    "    ret_df = pd.concat([ret_df, day_names, hours], axis=1)\n",
    "    return ret_df\n",
    "\n",
    "\n",
    "def split_by_seasons(mdf):\n",
    "    begin = 0\n",
    "    for i in range(mdf.shape[0] - 1):\n",
    "        if (mdf.index[i + 1] - mdf.index[i]).days > 50:\n",
    "            ret_df = mdf.iloc[begin:i, :]\n",
    "            print(\"Time Period: \",mdf.index[begin+1],\"->\",mdf.index[i])\n",
    "            begin = i\n",
    "            yield ret_df\n",
    "    print(\"Time Period: \",mdf.index[begin+1],\"->\",mdf.index[i])\n",
    "    yield mdf.iloc[begin:i, :]\n",
    "    \n",
    "    \n",
    "def find_max_f1():\n",
    "    mmax = 0\n",
    "    ival = 0\n",
    "    for i in range(1, 200):\n",
    "        check_y_hat =  Yhat*(1+0.01*i)\n",
    "        rmse = calc_rmse(Ytest,check_y_hat, graph=False)\n",
    "        tn, fp, fn, tp  = metrics.calc_tp_fp_rate(Ytest,check_y_hat,selected_value,binary=False,graph=False)\n",
    "        precision = tp / (tp + fp) \n",
    "        recall =  tp / (tp + fn)\n",
    "        f1 = 2 * (precision * recall) / (precision + recall)\n",
    "        if f1 > mmax:\n",
    "            mmax = f1\n",
    "            ival = i\n",
    "    return ival,mmax\n",
    "\n",
    "def aggregate_data(df,aggr,back,out):\n",
    "    HOUR = 60\n",
    "    \n",
    "    df = df.resample(\"%dMin\" % aggr).mean()\n",
    "    aggr = HOUR / aggr # aggr = 2\n",
    "    back = int(back * aggr)\n",
    "    out = int(out* aggr)\n",
    "        \n",
    "    return df,back,out\n",
    "\n",
    "\n",
    "def geometric_series(N, un0, q):\n",
    "    u = np.empty((N,))\n",
    "    u[0] = un0\n",
    "    u[1:] = q\n",
    "    return np.round(np.cumprod(u),0).astype(int)\n",
    "\n",
    "def geometric_series_with_back(back,un0,q):\n",
    "    for i in range(1,back):\n",
    "        if sum(geometric_series(i,un0,q))>back:\n",
    "            return geometric_series(i-1,un0,q)\n",
    "    return geometric_series(back-1,un0,q)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1,Y1=[],[]\n",
    "def create_final_dataset(df,perc_df=False,binary=False,**kwargs):\n",
    "    global first_reactor_index,mdfs,X1,Y1\n",
    "    mdfs = []\n",
    "    X,Y = [],[]\n",
    "    for mdf in [df]:\n",
    "\n",
    "                print(\"[LOG] Data Aggregation\")\n",
    "                #mdf, uback,uout = aggregate_data(mdf,kwargs[\"first_aggr\"],kwargs[\"back\"],kwargs[\"out\"])\n",
    "                uback,uout=    kwargs[\"back\"],kwargs[\"out\"]\n",
    "                if type(perc_df) != type(False):\n",
    "                    print(\"[LOG] Additional Dataset Merge\")\n",
    "\n",
    "                    df_min,df_max = min(mdf.index),max(mdf.index)\n",
    "\n",
    "                    cur = perc_df[df_min:df_max]\n",
    "                \n",
    "                    mdf[\"date_hour\"]=mdf.index\n",
    "                    mdf = mdf.merge(cur,on=\"date_hour\",how='left')\n",
    "                    mdf.index = mdf[\"date_hour\"]\n",
    "                    mdf = mdf.drop(columns=['date_hour'])\n",
    "\n",
    "                    mdf = add_one_hot(mdf)\n",
    "                    mdf = mdf.iloc[:,2:]\n",
    "\n",
    "                    mdf = mdf.fillna(-1)\n",
    "                    mdf = mdf.replace('-',-1)\n",
    "\n",
    "                    first_reactor_index = mdf.columns.get_loc('FeedFlowtoReactor1m3/hr')\n",
    "                mdfs.append(mdf)\n",
    "                #print(\"[LOG] series to supervised back \",uback,\" out \", uout)\n",
    "                #X1,Y1 = series_to_supervised(mdf.values, n_steps_in=uback , n_steps_out=uout,\n",
    "                #                            steps=kwargs[\"steps\"],binary=binary) \n",
    "                #comment = \"\"\n",
    "                #print(\"[LOG] Data Linearization\")\n",
    "                #X1,Y1, comment = lstme.linearize_dataset(mdf.values,steps_out=uout,size=uback,\n",
    "                #                           binary=binary,**kwargs)\n",
    "                X1,Y1, comment = lstme.special_linearize(mdf.values,steps_out=uout,size=uback,\n",
    "                                           binary=binary,**kwargs)\n",
    "                \n",
    "                X.append(X1)\n",
    "                Y.append(Y1)\n",
    "                print(\"[LOG] Finished season\")\n",
    "\n",
    "    return X,Y, comment\n",
    "\n",
    "def param_dict_generator():\n",
    "    for model in list(models.ModelType):\n",
    "        for optimizer in ['SGD',  'Adam']:\n",
    "            for epoch in  [50, 100]:\n",
    "                for batch in [20]:\n",
    "                    yield {'optimizer':optimizer,'model':model,'epoch':epoch,'batch':batch}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_series_size_2(mul,init,back):\n",
    "    if mul == 1:\n",
    "        return int(back/init)\n",
    "    result = back * (mul-1)\n",
    "    result = result / init\n",
    "    result+=1\n",
    "    result = math.log(result,mul)\n",
    "    return int(result)\n",
    "\n",
    "def calc_needed_back(mul,init,entrances):\n",
    "    back = 1\n",
    "    calced_size = calc_series_size(mul,init,back)\n",
    "    while calced_size < entrances:\n",
    "        back+=10\n",
    "        calced_size = calc_series_size(mul,init,back)\n",
    "\n",
    "    return back-1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X,Y,model,val,Yval,Yhat,Ytest = [],[],[],[],[],[],[]\n",
    "def run_exp(exp_params,perc_df=False):\n",
    "    global X,Y,df,model,Xval,Yval,Yhat,Ytest\n",
    "    print(\"[LOG] Creating Dataset\")\n",
    "    X,Y,comments = create_final_dataset(df[:],perc_df=perc_df, binary=binary,**exp_params)\n",
    "    exp_params[\"comments\"]+=str(comments)\n",
    "    print(\"[LOG] Normalizing\")\n",
    "    conX,conY = np.concatenate(X),np.concatenate(Y)\n",
    "    conX = np.asarray(conX).astype(np.float32)\n",
    "    conY = np.asarray(conY).astype(np.float32)\n",
    "    conX, conY, scalers = normalize(conX,conY ,binary=binary)  \n",
    "\n",
    "    print(\"[LOG] Spliting Data\")\n",
    "\n",
    "    Xtrain, Xval, Ytrain, Yval = train_test_split(conX, conY, test_size=0.2, random_state=42)\n",
    "    #if binary:\n",
    "    #    Xtrain,Ytrain = oversampler(Xtrain,Ytrain)\n",
    "    # 0.25 x 0.8 = 0.2\n",
    "    Xtrain, Xtest, Ytrain, Ytest = train_test_split(Xtrain, Ytrain, test_size=0.25, random_state=42)\n",
    "\n",
    "    model = models.make_model(exp_params['model'], conX, conY,opt=exp_params['optimizer'], binary=binary)\n",
    "    print(\"[LOG] Training\")\n",
    "    if np.isnan(np.min(conX)):\n",
    "        print(\"ERROR! contains nan\")\n",
    "        return \n",
    "\n",
    "    history = models.fit(Xtrain, Ytrain, Xtest, Ytest,model,\n",
    "                         epochs=exp_params['epoch'],batch_size= exp_params['batch'],verbose=1 if exp_params[\"graph\"] else 0,graph=exp_params[\"graph\"])\n",
    "    # In[7]\n",
    "    Yhat, Ytest = models.evaluate(model, Xval, Yval, scalers, binary=binary)\n",
    "    if not binary:\n",
    "        rmse = metrics.calc_rmse(Ytest,Yhat,graph=exp_params[\"graph\"])\n",
    "        exp_params[\"rmse\"] = rmse\n",
    "    else:\n",
    "        accuracy = metrics.calc_best_accuracy(Ytest,Yhat)\n",
    "        f1 = metrics.calc_best_f1(Ytest,Yhat)\n",
    "        _,_,_,auc = metrics.roc(Ytest,Yhat,graph=exp_params[\"graph\"])\n",
    "        print(accuracy,f1,auc)\n",
    "        exp_params[\"accuracy\"] = accuracy\n",
    "        exp_params[\"f1\"] = f1\n",
    "        exp_params[\"auc\"] = auc\n",
    "\n",
    "        \n",
    "    return exp_params\n",
    "\n",
    "\n",
    "def run_exp_variable(key_name,values):\n",
    "    res = []\n",
    "    print(\"START\")\n",
    "    importlib.reload(models)\n",
    "\n",
    "\n",
    "    #Multiplier = [round(1+i*0.1,1) for i in range(10)]\n",
    "    m_models = list(models.ModelType)\n",
    "    #key_name = 'model'\n",
    "    #values = m_models\n",
    "    result_list = []\n",
    "    with open(\"LSTME/%s/%s_%s\" % (experiment_name,var_name,key_name),\"w+\", newline='') as mfile:\n",
    "        w = csv.DictWriter(mfile, template_params.keys())\n",
    "        w.writeheader()\n",
    "        for exp_params in [dict(template_params) for i in values if not template_params.update({key_name:i})]:\n",
    "                print(exp_params)\n",
    "                return_list.append(run_exp(exp_param))\n",
    "    return result_list\n",
    "\n",
    "   \n",
    "def gen_num_to_locations(gen_num,back):\n",
    "        gen_num = [int(i) for i in gen_num.tolist()]\n",
    "        gen_num.append(back-sum(gen_num))\n",
    "        res_arr = []\n",
    "        cur = 0\n",
    "        for var in gen_num:\n",
    "            res_arr.append(cur+var)\n",
    "            cur=cur+var\n",
    "        return res_arr\n",
    "    \n",
    "def run_func(gen_num):\n",
    "        global template_params\n",
    "        print(gen_num)\n",
    "        res_arr = gen_num_to_locations(gen_num,template_params[\"back\"])\n",
    "        template_params[\"gen_num\"] = res_arr\n",
    "        res = run_exp(dict(template_params),perc_df=perc_df)\n",
    "\n",
    "        return res\n",
    "\n",
    "def series_maker(upper_limit):\n",
    "    return \"\".join(list(map(str,range(1,upper_limit))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment Started -  turkey\n",
      "[[1, 2, 3, 4, 5, 6, 7], [1, 1, 1, 2, 2, 2, 3], [1, 1, 1, 2, 4, 8, 15], [1, 1, 1, 2, 2, 2, 10], [4, 4, 4, 4, 4, 4, 4], [2, 2, 2, 2, 3, 3, 3], [1, 1, 1, 1, 1, 2, 2], [1, 1, 1, 2, 2, 2, 2]]\n",
      "[1 1 1 2 2 2 2]\n",
      "[LOG] Creating Dataset\n",
      "[LOG] Data Aggregation\n",
      "[LOG]Started Linearizing]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48a47f00eee34c989bfe82cf16fab07f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=25000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:00<00:00, 166.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[LOG] Finished season\n",
      "[LOG] Normalizing\n",
      "[LOG] Spliting Data\n",
      "ModelType.STACKED_LSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL: STACKED_LSTM  LossFunc:  mse  OptimizerL  SGD\n",
      "[LOG] Training\n",
      "{'first_aggr': 1, 'steps': 1, 'back': 48, 'out': 4, 'model': <ModelType.STACKED_LSTM: 2>, 'optimizer': 'SGD', 'limit': 15, 'multiplier': 1, 'init_size': 1, 'method': 'avg', 'epoch': 100, 'batch': 32, 'rmse': 3110.044372673805, 'accuracy': '', 'f1': '', 'auc': '', 'comments': '0:  Window Sizes: [8], [1, 1, 1, 2, 2, 2, 2, 37]', 'graph': False, 'gen_num': [1, 2, 3, 5, 7, 9, 11, 48], 'kfold': 5}\n",
      "[1 1 1 1 1 2 2]\n",
      "[LOG] Creating Dataset\n",
      "[LOG] Data Aggregation\n",
      "[LOG]Started Linearizing]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9548ee358ccc400faa2883dd26c0e79b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=25000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:00<00:00, 165.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[LOG] Finished season\n",
      "[LOG] Normalizing\n",
      "[LOG] Spliting Data\n",
      "ModelType.STACKED_LSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL: STACKED_LSTM  LossFunc:  mse  OptimizerL  SGD\n",
      "[LOG] Training\n",
      "{'first_aggr': 1, 'steps': 1, 'back': 48, 'out': 4, 'model': <ModelType.STACKED_LSTM: 2>, 'optimizer': 'SGD', 'limit': 15, 'multiplier': 1, 'init_size': 1, 'method': 'avg', 'epoch': 100, 'batch': 32, 'rmse': 2941.851457840793, 'accuracy': '', 'f1': '', 'auc': '', 'comments': '1:  Window Sizes: [8], [1, 1, 1, 1, 1, 2, 2, 39]', 'graph': False, 'gen_num': [1, 2, 3, 4, 5, 7, 9, 48], 'kfold': 5}\n",
      "[2 2 2 2 3 3 3]\n",
      "[LOG] Creating Dataset\n",
      "[LOG] Data Aggregation\n",
      "[LOG]Started Linearizing]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b12f49084e4249158d107f659e9cde65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=25000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:00<00:00, 178.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[LOG] Finished season\n",
      "[LOG] Normalizing\n",
      "[LOG] Spliting Data\n",
      "ModelType.STACKED_LSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL: STACKED_LSTM  LossFunc:  mse  OptimizerL  SGD\n",
      "[LOG] Training\n",
      "{'first_aggr': 1, 'steps': 1, 'back': 48, 'out': 4, 'model': <ModelType.STACKED_LSTM: 2>, 'optimizer': 'SGD', 'limit': 15, 'multiplier': 1, 'init_size': 1, 'method': 'avg', 'epoch': 100, 'batch': 32, 'rmse': 3426.951123083024, 'accuracy': '', 'f1': '', 'auc': '', 'comments': '2:  Window Sizes: [8], [2, 2, 2, 2, 3, 3, 3, 31]', 'graph': False, 'gen_num': [2, 4, 6, 8, 11, 14, 17, 48], 'kfold': 5}\n",
      "[4 4 4 4 4 4 4]\n",
      "[LOG] Creating Dataset\n",
      "[LOG] Data Aggregation\n",
      "[LOG]Started Linearizing]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "557b979873fc45eaa58f16aecdf9b1c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=25000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:00<00:00, 174.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[LOG] Finished season\n",
      "[LOG] Normalizing\n",
      "[LOG] Spliting Data\n",
      "ModelType.STACKED_LSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL: STACKED_LSTM  LossFunc:  mse  OptimizerL  SGD\n",
      "[LOG] Training\n",
      "{'first_aggr': 1, 'steps': 1, 'back': 48, 'out': 4, 'model': <ModelType.STACKED_LSTM: 2>, 'optimizer': 'SGD', 'limit': 15, 'multiplier': 1, 'init_size': 1, 'method': 'avg', 'epoch': 100, 'batch': 32, 'rmse': 3829.200961036127, 'accuracy': '', 'f1': '', 'auc': '', 'comments': '3:  Window Sizes: [8], [4, 4, 4, 4, 4, 4, 4, 20]', 'graph': False, 'gen_num': [4, 8, 12, 16, 20, 24, 28, 48], 'kfold': 5}\n",
      "[ 1  1  1  2  2  2 10]\n",
      "[LOG] Creating Dataset\n",
      "[LOG] Data Aggregation\n",
      "[LOG]Started Linearizing]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ab8d08d105244fdbd7dbd0316e84971",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=25000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:00<00:00, 172.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[LOG] Finished season\n",
      "[LOG] Normalizing\n",
      "[LOG] Spliting Data\n",
      "ModelType.STACKED_LSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL: STACKED_LSTM  LossFunc:  mse  OptimizerL  SGD\n",
      "[LOG] Training\n",
      "{'first_aggr': 1, 'steps': 1, 'back': 48, 'out': 4, 'model': <ModelType.STACKED_LSTM: 2>, 'optimizer': 'SGD', 'limit': 15, 'multiplier': 1, 'init_size': 1, 'method': 'avg', 'epoch': 100, 'batch': 32, 'rmse': 3148.4288145041487, 'accuracy': '', 'f1': '', 'auc': '', 'comments': '4:  Window Sizes: [8], [1, 1, 1, 2, 2, 2, 10, 29]', 'graph': False, 'gen_num': [1, 2, 3, 5, 7, 9, 19, 48], 'kfold': 5}\n",
      "[ 1  1  1  2  4  8 15]\n",
      "[LOG] Creating Dataset\n",
      "[LOG] Data Aggregation\n",
      "[LOG]Started Linearizing]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "338b1833519d435690f4581caa054c42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=25000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:00<00:00, 174.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[LOG] Finished season\n",
      "[LOG] Normalizing\n",
      "[LOG] Spliting Data\n",
      "ModelType.STACKED_LSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL: STACKED_LSTM  LossFunc:  mse  OptimizerL  SGD\n",
      "[LOG] Training\n",
      "{'first_aggr': 1, 'steps': 1, 'back': 48, 'out': 4, 'model': <ModelType.STACKED_LSTM: 2>, 'optimizer': 'SGD', 'limit': 15, 'multiplier': 1, 'init_size': 1, 'method': 'avg', 'epoch': 100, 'batch': 32, 'rmse': 3108.3732079658644, 'accuracy': '', 'f1': '', 'auc': '', 'comments': '5:  Window Sizes: [8], [1, 1, 1, 2, 4, 8, 15, 16]', 'graph': False, 'gen_num': [1, 2, 3, 5, 9, 17, 32, 48], 'kfold': 5}\n",
      "[1 1 1 2 2 2 3]\n",
      "[LOG] Creating Dataset\n",
      "[LOG] Data Aggregation\n",
      "[LOG]Started Linearizing]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13a9218dbb4e44278a462879fc694cb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=25000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:00<00:00, 181.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[LOG] Finished season\n",
      "[LOG] Normalizing\n",
      "[LOG] Spliting Data\n",
      "ModelType.STACKED_LSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL: STACKED_LSTM  LossFunc:  mse  OptimizerL  SGD\n",
      "[LOG] Training\n"
     ]
    }
   ],
   "source": [
    "\n",
    "importlib.reload(lstme)\n",
    "importlib.reload(models)\n",
    "import pathlib\n",
    "\n",
    "\n",
    "keys = [\"first_aggr\", \"steps\", \"back\", \"out\", \"model\", \"optimizer\", \"limit\",\n",
    "        \"multiplier\", \"init_size\", \"method\", \"epoch\", \"batch\", \"rmse\",\n",
    "        \"accuracy\", \"f1\", \"auc\", \"comments\", \"graph\", \"gen_num\", \"kfold\"]\n",
    "\n",
    "kfold_num = 5\n",
    "ammonia_limit = 15\n",
    "epochs = 100\n",
    "comment = \"\"\n",
    "perc_df = False\n",
    "datasets = []\n",
    "datasets = [\"ammonia\"]\n",
    "\n",
    "for dataset_name in datasets:\n",
    "    wanted_size = 5000\n",
    "    out_predict = 4\n",
    "\n",
    "    print(\"Experiment Started - \", dataset_name)\n",
    "    variant = \"complicated\"\n",
    "    mode = \"a\"\n",
    "    pathlib.Path(\"LSTME/\" + variant).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    if dataset_name == \"turkey\":\n",
    "        adf = pd.read_pickle(\"datasets/turkey_power.pkl\")\n",
    "        mfile = open(\"LSTME/%s/turkey_power.csv\" % variant, mode, newline='')\n",
    "\n",
    "    elif dataset_name == \"nitrate\":\n",
    "        adf = pd.read_pickle(\"nitrate_new.pkl\")\n",
    "        mfile = open(\"LSTME/%s/nitrate.csv\" % variant, mode, newline='')\n",
    "        \n",
    "    elif dataset_name == \"ammonia\":\n",
    "        adf = pd.read_pickle(\"temp_dataframe.pkl\")\n",
    "        mfile = open(\"LSTME/%s/ammonia.csv\" % variant, mode, newline='')\n",
    "\n",
    "    else:\n",
    "        raise Exception()\n",
    "        \n",
    "        \n",
    "    w = csv.DictWriter(mfile, keys)\n",
    "    if (mode == \"w\"):\n",
    "        w.writeheader()\n",
    "\n",
    "    binary = False\n",
    "    wanted_size = adf.shape[0] // kfold_num\n",
    "\n",
    "    rmse_list = []\n",
    "    series = []\n",
    "\n",
    "    window_size = 8\n",
    "\n",
    "    for window_size in range(5,10,2):\n",
    "        # This is where parameters are selected\n",
    "        # To do multiple experiments, change template_params every time\n",
    "        template_params = {\"first_aggr\": 1, \"steps\": 1, \"back\": window_size, \"out\": out_predict,\n",
    "                           \"model\": models.ModelType.STACKED_LSTM, \"optimizer\": \"SGD\",\n",
    "                           \"limit\": ammonia_limit, \"multiplier\": 1, \"init_size\": 1, \"method\": aggr_type,\n",
    "                           \"epoch\": epochs, \"batch\": 32, \"rmse\": \"\", \"accuracy\": \"\", \"f1\": \"\", \"auc\": \"\",\n",
    "                           \"comments\": f\"{idx}: {comment} \",\n",
    "                           \"graph\": False, \"gen_num\": \"\", \"kfold\": kfold_num}\n",
    "\n",
    "        # ignore this param\n",
    "        series = np.array([1]*window_size)\n",
    "        res = run_func(series)\n",
    "        \n",
    "        print(res)\n",
    "        if res is None:\n",
    "            print(\"Returned Error\")\n",
    "            continue\n",
    "        final_dict = res\n",
    "        try:\n",
    "            rmse_list.append(final_dict[\"rmse\"])\n",
    "        except:\n",
    "            traceback.print_exc()\n",
    "            pass\n",
    "\n",
    "        if w is not None:\n",
    "            w.writerow(final_dict)\n",
    "            mfile.flush()\n",
    "\n",
    "mfile.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TF",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
